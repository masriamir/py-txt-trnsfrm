name: Performance Testing

on:
  workflow_dispatch:
    inputs:
      target_environment:
        description: 'Target environment for performance testing'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
        - local
      test_duration:
        description: 'Test duration in minutes'
        required: false
        default: '5'
        type: string
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '10'
        type: string
      performance_baseline:
        description: 'Compare against performance baseline'
        required: false
        default: true
        type: boolean
  schedule:
    # Run performance tests weekly on Sunday at 3:00 AM UTC
    - cron: '0 3 * * 0'
  pull_request:
    branches: [main]
    paths:
      - 'app/**'
      - 'requirements.txt'
      - 'pyproject.toml'
      - 'Dockerfile'
      - 'gunicorn.conf.py'
    types: [opened, synchronize]

env:
  PYTHON_VERSION: "3.13"
  UV_VERSION: ">=0.8.11,<0.9.0"

jobs:
  # Performance testing setup and validation
  setup-performance-testing:
    name: Setup Performance Testing
    runs-on: ubuntu-latest
    outputs:
      target_url: ${{ steps.setup.outputs.target_url }}
      test_config: ${{ steps.setup.outputs.test_config }}
      should_run_load_tests: ${{ steps.setup.outputs.should_run_load_tests }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup test configuration
      id: setup
      run: |
        # Determine target environment and configuration
        TARGET_ENV="${{ inputs.target_environment || 'staging' }}"
        DURATION="${{ inputs.test_duration || '5' }}"
        USERS="${{ inputs.concurrent_users || '10' }}"
        
        echo "Target environment: $TARGET_ENV"
        echo "Test duration: $DURATION minutes"
        echo "Concurrent users: $USERS"
        
        # Set target URL based on environment
        case "$TARGET_ENV" in
          "production")
            TARGET_URL="https://py-txt-trnsfrm.herokuapp.com"
            ;;
          "staging")
            TARGET_URL="https://py-txt-trnsfrm-staging.herokuapp.com"
            ;;
          "local")
            TARGET_URL="http://localhost:5000"
            ;;
          *)
            echo "Unknown environment: $TARGET_ENV"
            exit 1
            ;;
        esac
        
        # Create test configuration
        TEST_CONFIG=$(cat << EOF
        {
          "target_url": "$TARGET_URL",
          "duration_minutes": $DURATION,
          "concurrent_users": $USERS,
          "environment": "$TARGET_ENV"
        }
        EOF
        )
        
        echo "target_url=$TARGET_URL" >> $GITHUB_OUTPUT
        echo "test_config=$TEST_CONFIG" >> $GITHUB_OUTPUT
        
        # Determine if we should run comprehensive load tests
        SHOULD_RUN_LOAD_TESTS="true"
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          SHOULD_RUN_LOAD_TESTS="false"  # Light testing for PRs
        fi
        echo "should_run_load_tests=$SHOULD_RUN_LOAD_TESTS" >> $GITHUB_OUTPUT

    - name: Validate target environment
      run: |
        TARGET_URL="${{ steps.setup.outputs.target_url }}"
        
        if [[ "${{ inputs.target_environment || 'staging' }}" != "local" ]]; then
          echo "Checking if target environment is accessible..."
          if curl -f "$TARGET_URL/health" -m 30; then
            echo "✅ Target environment is accessible"
          else
            echo "❌ Target environment is not accessible"
            echo "::error::Cannot reach $TARGET_URL - performance tests will be skipped"
            exit 1
          fi
        fi

  # Setup local environment for testing (if needed)
  setup-local-environment:
    name: Setup Local Test Environment
    runs-on: ubuntu-latest
    needs: setup-performance-testing
    if: inputs.target_environment == 'local'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build application image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        load: true
        tags: py-txt-trnsfrm:performance-test
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Start application container
      run: |
        docker run -d \
          --name performance-test-app \
          -p 5000:5000 \
          -e FLASK_ENV=production \
          -e LOG_LEVEL=warning \
          py-txt-trnsfrm:performance-test

    - name: Wait for application to be ready
      run: |
        echo "Waiting for application to start..."
        for i in {1..30}; do
          if curl -f http://localhost:5000/health -m 5; then
            echo "✅ Application is ready"
            break
          fi
          echo "Attempt $i failed, retrying in 2 seconds..."
          sleep 2
        done

  # Basic performance tests
  basic-performance-tests:
    name: Basic Performance Tests
    runs-on: ubuntu-latest
    needs: [setup-performance-testing, setup-local-environment]
    if: always() && needs.setup-performance-testing.result == 'success'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        version: ${{ env.UV_VERSION }}

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Install dependencies and performance testing tools
      run: |
        uv sync --group dev --group test
        uv add locust pytest-benchmark requests

    - name: Create performance test script
      run: |
        mkdir -p tests/performance
        
        cat > tests/performance/test_api_performance.py << 'EOF'
        """API Performance Tests"""
        import pytest
        import requests
        import time
        import statistics
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        BASE_URL = "${{ needs.setup-performance-testing.outputs.target_url }}"
        
        
        class TestAPIPerformance:
            """API performance test suite"""
            
            def test_health_endpoint_response_time(self, benchmark):
                """Test health endpoint response time"""
                def health_check():
                    response = requests.get(f"{BASE_URL}/health", timeout=5)
                    return response
                
                result = benchmark(health_check)
                assert result.status_code == 200
            
            def test_home_page_response_time(self, benchmark):
                """Test home page response time"""
                def get_home_page():
                    response = requests.get(f"{BASE_URL}/", timeout=10)
                    return response
                
                result = benchmark(get_home_page)
                assert result.status_code == 200
            
            def test_transformation_endpoint_performance(self, benchmark):
                """Test transformation endpoint performance"""
                def transform_text():
                    data = {
                        'text': 'Hello World! This is a performance test.',
                        'transformation': 'alternate_case'
                    }
                    response = requests.post(f"{BASE_URL}/transform", data=data, timeout=10)
                    return response
                
                result = benchmark(transform_text)
                assert result.status_code == 200
            
            @pytest.mark.skipif(
                "${{ github.event_name }}" == "pull_request",
                reason="Skip concurrent tests in PR to reduce load"
            )
            def test_concurrent_requests(self):
                """Test concurrent request handling"""
                def make_request():
                    start_time = time.time()
                    response = requests.get(f"{BASE_URL}/health", timeout=10)
                    end_time = time.time()
                    return response.status_code, end_time - start_time
                
                # Test with multiple concurrent requests
                concurrent_users = min(int("${{ inputs.concurrent_users || '5' }}"), 20)  # Limit for PR tests
                
                with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                    futures = [executor.submit(make_request) for _ in range(concurrent_users * 2)]
                    results = [future.result() for future in as_completed(futures)]
                
                # Analyze results
                response_times = [result[1] for result in results]
                status_codes = [result[0] for result in results]
                
                # All requests should succeed
                assert all(code == 200 for code in status_codes), f"Some requests failed: {status_codes}"
                
                # Response times should be reasonable
                avg_response_time = statistics.mean(response_times)
                max_response_time = max(response_times)
                
                print(f"Average response time: {avg_response_time:.3f}s")
                print(f"Max response time: {max_response_time:.3f}s")
                print(f"95th percentile: {statistics.quantiles(response_times, n=20)[18]:.3f}s")
                
                # Performance assertions
                assert avg_response_time < 2.0, f"Average response time too high: {avg_response_time:.3f}s"
                assert max_response_time < 5.0, f"Max response time too high: {max_response_time:.3f}s"
        EOF

    - name: Run basic performance tests
      run: |
        mkdir -p reports/performance
        
        uv run pytest tests/performance/test_api_performance.py \
          --benchmark-json=reports/performance/benchmark_results.json \
          --benchmark-histogram=reports/performance/benchmark_histogram \
          --benchmark-only \
          -v

    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: basic-performance-results
        path: |
          reports/performance/
        retention-days: 30

  # Load testing with Locust
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: [setup-performance-testing, basic-performance-tests]
    if: needs.setup-performance-testing.outputs.should_run_load_tests == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        version: ${{ env.UV_VERSION }}

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Install locust
      run: uv add locust

    - name: Create locust test file
      run: |
        mkdir -p tests/performance
        
        cat > tests/performance/locustfile.py << 'EOF'
        """Locust load testing configuration"""
        from locust import HttpUser, task, between
        import random
        
        
        class WebsiteUser(HttpUser):
            wait_time = between(1, 3)  # Wait 1-3 seconds between requests
            
            def on_start(self):
                """Called when a user starts"""
                # Test basic connectivity
                self.client.get("/health")
            
            @task(3)
            def view_homepage(self):
                """Visit the homepage"""
                self.client.get("/")
            
            @task(2)
            def health_check(self):
                """Health check endpoint"""
                self.client.get("/health")
            
            @task(1)
            def transform_text(self):
                """Transform text using API"""
                transformations = ['alternate_case', 'backwards', 'rot13']
                texts = [
                    "Hello World!",
                    "This is a performance test",
                    "Testing load with multiple users",
                    "Flask application stress test"
                ]
                
                data = {
                    'text': random.choice(texts),
                    'transformation': random.choice(transformations)
                }
                
                response = self.client.post("/transform", data=data)
                
                # Validate response
                if response.status_code != 200:
                    print(f"Transform request failed: {response.status_code}")
        EOF

    - name: Run load tests
      run: |
        TARGET_URL="${{ needs.setup-performance-testing.outputs.target_url }}"
        DURATION="${{ inputs.test_duration || '5' }}"
        USERS="${{ inputs.concurrent_users || '10' }}"
        
        mkdir -p reports/performance
        
        echo "Starting load test..."
        echo "Target: $TARGET_URL"
        echo "Duration: ${DURATION} minutes"
        echo "Users: $USERS"
        
        # Run locust in headless mode
        uv run locust \
          -f tests/performance/locustfile.py \
          --host="$TARGET_URL" \
          --users="$USERS" \
          --spawn-rate=2 \
          --run-time="${DURATION}m" \
          --headless \
          --html=reports/performance/load_test_report.html \
          --csv=reports/performance/load_test \
          --logfile=reports/performance/load_test.log \
          --loglevel=INFO

    - name: Process load test results
      run: |
        echo "::group::Load Test Results Summary"
        
        if [ -f "reports/performance/load_test_stats.csv" ]; then
          echo "Request Statistics:"
          cat reports/performance/load_test_stats.csv
          echo ""
          
          # Extract key metrics
          TOTAL_REQUESTS=$(tail -n 1 reports/performance/load_test_stats.csv | cut -d',' -f2)
          FAILURE_RATE=$(tail -n 1 reports/performance/load_test_stats.csv | cut -d',' -f3)
          AVG_RESPONSE_TIME=$(tail -n 1 reports/performance/load_test_stats.csv | cut -d',' -f6)
          
          echo "Key Metrics:"
          echo "- Total Requests: $TOTAL_REQUESTS"
          echo "- Failure Rate: $FAILURE_RATE"
          echo "- Average Response Time: ${AVG_RESPONSE_TIME}ms"
          
          # Performance thresholds
          if (( $(echo "$FAILURE_RATE > 5" | bc -l) )); then
            echo "::error::High failure rate detected: $FAILURE_RATE%"
            exit 1
          fi
          
          if (( $(echo "$AVG_RESPONSE_TIME > 2000" | bc -l) )); then
            echo "::warning::High average response time: ${AVG_RESPONSE_TIME}ms"
          fi
        fi
        
        echo "::endgroup::"

    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-results
        path: |
          reports/performance/
        retention-days: 30

  # Performance analysis and reporting
  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: [setup-performance-testing, basic-performance-tests, load-testing]
    if: always() && needs.setup-performance-testing.result == 'success'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download performance results
      uses: actions/download-artifact@v4
      with:
        pattern: "*-performance-results"
        merge-multiple: true
        path: reports/performance/

    - name: Download load test results
      if: needs.load-testing.result == 'success'
      uses: actions/download-artifact@v4
      with:
        name: load-test-results
        path: reports/performance/

    - name: Generate performance report
      run: |
        mkdir -p reports/performance
        
        cat > reports/performance/performance_summary.md << 'EOF'
        # Performance Test Report
        
        **Generated on:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        **Target Environment:** ${{ inputs.target_environment || 'staging' }}
        **Test Configuration:** ${{ needs.setup-performance-testing.outputs.test_config }}
        
        ## Test Results Summary
        
        EOF
        
        # Add basic performance test results
        if [ -f "reports/performance/benchmark_results.json" ]; then
          echo "### Basic Performance Tests" >> reports/performance/performance_summary.md
          echo "Benchmark tests completed successfully. See \`benchmark_results.json\` for detailed metrics." >> reports/performance/performance_summary.md
          echo "" >> reports/performance/performance_summary.md
        fi
        
        # Add load test results
        if [ -f "reports/performance/load_test_stats.csv" ]; then
          echo "### Load Test Results" >> reports/performance/performance_summary.md
          echo "\`\`\`" >> reports/performance/performance_summary.md
          cat reports/performance/load_test_stats.csv >> reports/performance/performance_summary.md
          echo "\`\`\`" >> reports/performance/performance_summary.md
          echo "" >> reports/performance/performance_summary.md
        fi
        
        # Add recommendations
        echo "### Recommendations" >> reports/performance/performance_summary.md
        echo "- Monitor response times during peak usage" >> reports/performance/performance_summary.md
        echo "- Consider scaling if failure rates exceed 1%" >> reports/performance/performance_summary.md
        echo "- Optimize endpoints with response times > 1000ms" >> reports/performance/performance_summary.md

    - name: Create performance issue on degradation
      if: failure() && github.event_name == 'schedule'
      uses: actions/github-script@v7
      with:
        script: |
          const title = `⚡ Performance Degradation Detected - ${new Date().toISOString().split('T')[0]}`;
          const body = `
          Performance tests have detected potential issues that require attention.
          
          **Test Configuration:**
          - **Environment:** ${{ inputs.target_environment || 'staging' }}
          - **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          - **Date:** ${new Date().toISOString()}
          
          **Next Steps:**
          1. Review the performance reports in the workflow artifacts
          2. Analyze response time and throughput metrics
          3. Check for infrastructure issues
          4. Consider performance optimizations
          
          **Artifacts:**
          Download the performance reports from the workflow run for detailed analysis.
          `;
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['performance', 'monitoring', 'needs-investigation']
          });

    - name: Upload final performance report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-analysis-report
        path: |
          reports/performance/performance_summary.md
          reports/performance/
        retention-days: 90